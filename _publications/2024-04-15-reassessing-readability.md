---
title: "Reassessing Java Code Readability Models with a Human-Centered Approach"
authors: '<i>Agnia Sergeyuk, Olga Lvova, Sergey Titov, Anastasiia Serova, Farid Bagirov, Evgeniia Kirillova, and Timofey Bryksin</i>'
status: "published"
collection: publications
permalink: /publications/2024-04-15-reassessing-readability
date: 2024-04-15
venue: "the proceedings of <b>ICPC'24</b>"
level: 'A'
pdf: 'https://arxiv.org/abs/2401.14936'
paperurl: 'https://doi.org/10.1145/3643916.3644435'
counter_id: 'C52'
data: 'https://zenodo.org/records/10550937'
abstract: "<p><b>Abstract</b>. To ensure that Large Language Models (LLMs) effectively support user productivity, they need to be adjusted. Existing Code Readability (CR) models can guide this alignment. However, there are concerns about their relevance in modern software engineering since they often miss the developers' notion of readability and rely on outdated code. This research assesses existing Java CR models for LLM adjustments, measuring the correlation between their and developers' evaluations of AI-generated Java code. Using the Repertory Grid Technique with 15 developers, we identified 12 key code aspects influencing CR that were consequently assessed by 390 programmers when labeling 120 AI-generated snippets. Our findings indicate that when AI generates concise and executable code, it is often considered readable by CR models and developers. However, a limited correlation between these evaluations underscores the importance of future research on learning objectives for adjusting LLMs and on the aspects influencing CR evaluations included in predictive models.</p>"
---